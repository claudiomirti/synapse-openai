{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "  \"name\": \"synapseml\",\r\n",
        "  \"conf\": {\r\n",
        "      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.11.0,org.apache.spark:spark-avro_2.12:3.3.1\",\r\n",
        "      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\r\n",
        "      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind\",\r\n",
        "      \"spark.yarn.user.classpath.first\": \"true\",\r\n",
        "      \"spark.sql.parquet.enableVectorizedReader\": \"false\",\r\n",
        "      \"spark.sql.legacy.replaceDatabricksSparkAvro.enabled\": \"true\"\r\n",
        "  }\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from synapse.ml.core.platform import running_on_synapse, find_secret\r\n",
        "\r\n",
        "# Bootstrap Spark Session\r\n",
        "spark = SparkSession.builder.getOrCreate()\r\n",
        "\r\n",
        "if running_on_synapse():\r\n",
        "    from notebookutils.visualization import display\r\n",
        "\r\n",
        "# Fill in the following lines with your service information\r\n",
        "service_name = \"xxxxxxx\" # please replace this with your Service Name\r\n",
        "deployment_name = \"text-davinci-003\"\r\n",
        "deployment_name_embeddings = \"text-search-ada-doc-001\"\r\n",
        "deployment_name_embeddings_query = \"text-search-ada-query-001\"\r\n",
        "\r\n",
        "key = find_secret = \"xxxxxxxx\"  # please replace this with your key as a string\r\n",
        "\r\n",
        "assert key is not None and service_name is not None"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(\r\n",
        "    [\r\n",
        "        (\"Hello my name is\",),\r\n",
        "        (\"The best code is code thats\",),\r\n",
        "        (\"SynapseML is \",),\r\n",
        "    ]\r\n",
        ").toDF(\"prompt\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from synapse.ml.cognitive import OpenAICompletion\r\n",
        "\r\n",
        "completion = (\r\n",
        "    OpenAICompletion()\r\n",
        "    .setSubscriptionKey(key)\r\n",
        "    .setDeploymentName(deployment_name)\r\n",
        "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\r\n",
        "    .setMaxTokens(200)\r\n",
        "    .setPromptCol(\"prompt\")\r\n",
        "    .setErrorCol(\"error\")\r\n",
        "    .setOutputCol(\"completions\")\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "completed_df = completion.transform(df).cache()\r\n",
        "display(\r\n",
        "    completed_df.select(\r\n",
        "        col(\"prompt\"),\r\n",
        "        col(\"error\"),\r\n",
        "        col(\"completions.choices.text\").getItem(0).alias(\"text\"),\r\n",
        "    )\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_df = spark.createDataFrame(\r\n",
        "    [\r\n",
        "        ([\"The time has come\", \"Pleased to\", \"Today stocks\", \"Here's to\"],),\r\n",
        "        ([\"The only thing\", \"Ask not what\", \"Every litter\", \"I am\"],),\r\n",
        "    ]\r\n",
        ").toDF(\"batchPrompt\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_completion = (\r\n",
        "    OpenAICompletion()\r\n",
        "    .setSubscriptionKey(key)\r\n",
        "    .setDeploymentName(deployment_name)\r\n",
        "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\r\n",
        "    .setMaxTokens(200)\r\n",
        "    .setBatchPromptCol(\"batchPrompt\")\r\n",
        "    .setErrorCol(\"error\")\r\n",
        "    .setOutputCol(\"completions\")\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completed_batch_df = batch_completion.transform(batch_df).cache()\r\n",
        "display(completed_batch_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StringType\r\n",
        "from synapse.ml.stages import FixedMiniBatchTransformer\r\n",
        "from synapse.ml.core.spark import FluentAPI\r\n",
        "\r\n",
        "completed_autobatch_df = (\r\n",
        "    df.coalesce(\r\n",
        "        1\r\n",
        "    )  # Force a single partition so that our little 4-row dataframe makes a batch of size 4, you can remove this step for large datasets\r\n",
        "    .mlTransform(FixedMiniBatchTransformer(batchSize=4))\r\n",
        "    .withColumnRenamed(\"prompt\", \"batchPrompt\")\r\n",
        "    .mlTransform(batch_completion)\r\n",
        ")\r\n",
        "\r\n",
        "display(completed_autobatch_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translate_df = spark.createDataFrame(\r\n",
        "    [\r\n",
        "        (\"Japanese: Ookina hako \\nEnglish: Big box \\nJapanese: Midori tako\\nEnglish:\",),\r\n",
        "        (\r\n",
        "            \"French: Quel heure et il au Montreal? \\nEnglish: What time is it in Montreal? \\nFrench: Ou est le poulet? \\nEnglish:\",\r\n",
        "        ),\r\n",
        "    ]\r\n",
        ").toDF(\"prompt\")\r\n",
        "\r\n",
        "display(completion.transform(translate_df))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_df = spark.createDataFrame(\r\n",
        "    [\r\n",
        "        (\r\n",
        "            \"Q: Where is the Grand Canyon?\\nA: The Grand Canyon is in Arizona.\\n\\nQ: What is the weight of the Burj Khalifa in kilograms?\\nA:\",\r\n",
        "        )\r\n",
        "    ]\r\n",
        ").toDF(\"prompt\")\r\n",
        "\r\n",
        "display(completion.transform(qa_df))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from synapse.ml.cognitive import OpenAIEmbedding\r\n",
        "\r\n",
        "embedding = (\r\n",
        "    OpenAIEmbedding()\r\n",
        "    .setSubscriptionKey(key)\r\n",
        "    .setDeploymentName(deployment_name_embeddings)\r\n",
        "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\r\n",
        "    .setTextCol(\"combined\")\r\n",
        "    .setErrorCol(\"error\")\r\n",
        "    .setOutputCol(\"embeddings\")\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "\r\n",
        "df = spark.read.options(inferSchema=\"True\", delimiter=\",\", header=True).csv(\r\n",
        "    \"wasbs://publicwasb@mmlspark.blob.core.windows.net/fine_food_reviews_1k.csv\"\r\n",
        ")\r\n",
        "\r\n",
        "df = df.withColumn(\r\n",
        "    \"combined\",\r\n",
        "    F.format_string(\"Title: %s; Content: %s\", F.trim(df.Summary), F.trim(df.Text)),\r\n",
        ")\r\n",
        "\r\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "\r\n",
        "completed_df = embedding.transform(df).cache()\r\n",
        "display(completed_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "matrix = np.array(completed_df.select(\"embeddings\").collect())[:, 0, :]\r\n",
        "matrix.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Create a t-SNE model and transform the data\r\n",
        "tsne = TSNE(\r\n",
        "    n_components=2, perplexity=15, random_state=42, init=\"random\", learning_rate=200\r\n",
        ")\r\n",
        "vis_dims = tsne.fit_transform(matrix)\r\n",
        "vis_dims.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "scores = np.array(completed_df.select(\"Score\").collect()).reshape(-1)\r\n",
        "\r\n",
        "colors = [\"red\", \"darkorange\", \"gold\", \"turquoise\", \"darkgreen\"]\r\n",
        "x = [x for x, y in vis_dims]\r\n",
        "y = [y for x, y in vis_dims]\r\n",
        "color_indices = scores - 1\r\n",
        "\r\n",
        "colormap = matplotlib.colors.ListedColormap(colors)\r\n",
        "plt.scatter(x, y, c=color_indices, cmap=colormap, alpha=0.3)\r\n",
        "for score in [0, 1, 2, 3, 4]:\r\n",
        "    avg_x = np.array(x)[scores - 1 == score].mean()\r\n",
        "    avg_y = np.array(y)[scores - 1 == score].mean()\r\n",
        "    color = colors[score]\r\n",
        "    plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\r\n",
        "\r\n",
        "plt.title(\"Amazon ratings visualized in language using t-SNE\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_query = (\r\n",
        "    OpenAIEmbedding()\r\n",
        "    .setSubscriptionKey(key)\r\n",
        "    .setDeploymentName(deployment_name_embeddings_query)\r\n",
        "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\r\n",
        "    .setTextCol(\"query\")\r\n",
        "    .setErrorCol(\"error\")\r\n",
        "    .setOutputCol(\"embeddings\")\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_df = (\r\n",
        "    spark.createDataFrame(\r\n",
        "        [\r\n",
        "            (\r\n",
        "                0,\r\n",
        "                \"desserts\",\r\n",
        "            ),\r\n",
        "            (\r\n",
        "                1,\r\n",
        "                \"disgusting\",\r\n",
        "            ),\r\n",
        "        ]\r\n",
        "    )\r\n",
        "    .toDF(\"id\", \"query\")\r\n",
        "    .withColumn(\"id\", F.col(\"id\").cast(\"int\"))\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completed_query_df = embedding_query.transform(query_df).cache()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from synapse.ml.nn import *\r\n",
        "\r\n",
        "knn = (\r\n",
        "    KNN()\r\n",
        "    .setFeaturesCol(\"embeddings\")\r\n",
        "    .setValuesCol(\"id\")\r\n",
        "    .setOutputCol(\"output\")\r\n",
        "    .setK(10)\r\n",
        ")  # top-k for retrieval\r\n",
        "\r\n",
        "knn_index = knn.fit(completed_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_matches = knn_index.transform(completed_query_df).cache()\r\n",
        "\r\n",
        "df_result = (\r\n",
        "    df_matches.withColumn(\"match\", F.explode(\"output\"))\r\n",
        "    .join(df, df[\"id\"] == F.col(\"match.value\"))\r\n",
        "    .select(\"query\", F.col(\"combined\"), \"match.distance\")\r\n",
        ")\r\n",
        "\r\n",
        "display(df_result)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}